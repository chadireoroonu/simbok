import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta
import re
import google.generativeai as genai

st.set_page_config(page_title="ë‰´ìŠ¤ ì •ë¦¬ë´‡", page_icon="ğŸ› ï¸")

# AI ê°€ê³µ í•¨ìˆ˜
def generate_narration(api_key, text):
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('models/gemini-1.5-flash-8b')
        
        # í–¥í›„ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •í•˜ê¸°
        prompt = f"""
        ë‹¹ì‹ ì€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ ì•„ì£¼ ì¬ë¯¸ìˆê³  ê·€ì— ì™ì™ ë“¤ì–´ì˜¤ê²Œ ì „ë‹¬í•˜ëŠ” ì „ë¬¸ ë‰´ìŠ¤ ë‚˜ë ˆì´í„°ì…ë‹ˆë‹¤.
        ì•„ë˜ ë‰´ìŠ¤ ë³¸ë¬¸ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹œì²­ìì—ê²Œ ì§ì ‘ ì´ì•¼ê¸°í•˜ëŠ” ë“¯í•œ êµ¬ì–´ì²´ ìŠ¤íƒ€ì¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
        ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ í•µì‹¬ë§Œ ì§šì–´ì„œ 3ë¬¸ì¥ ì •ë„ë¡œ ê°€ê³µí•´ ì£¼ì„¸ìš”.
        
        ë‰´ìŠ¤ ë³¸ë¬¸:
        {text}
        """
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"AI ê°€ê³µ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}"

# UI
st.title("ë‰´ìŠ¤ ìš”ì•½ê¸°")
st.write("ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì„œ AIê°€ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    keyword = st.text_input("í‚¤ì›Œë“œ", value="í‚¤ì›Œë“œ")
    max_pages = st.number_input("ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜", min_value=1, max_value=20, value=5)
    
    st.divider()
    st.subheader("ğŸ”‘ AI ì„¤ì •")
    google_api_key = st.text_input("Google API Key", type="password", placeholder="API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš” ğŸ—ï¸")
    st.caption("ë°œê¸‰ì²˜: [Google AI Studio](https://aistudio.google.com/app/apikey)")
    
    st.divider()
    st.subheader("ğŸ“… ì¡°íšŒ ê¸°ê°„ ì„¤ì •")
    today = datetime.now()
    seven_days_ago = today - timedelta(days=7)
    date_range = st.date_input("ì¡°íšŒ ì‹œì‘ì¼ - ì¢…ë£Œì¼", value=(seven_days_ago, today), max_value=today)

# ë‰´ìŠ¤ í¬ë¡¤ë§
def crawl_news(keyword, pages):
    article_list = []
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}
    
    for page in range(1, pages + 1):
        url = f"https://search.daum.net/search?w=news&q={keyword}&p={page}&f=sort&sort=rec"
        try:
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            news_items = soup.find_all('li', {'data-docid': True}) or soup.select("ul.c-list-basic > li")

            for item in news_items:
                title_tag = item.select_one("div.item-title strong.tit-g a") or item.select_one("a.el-title")
                title = title_tag.get_text(strip=True) if title_tag else ""
                link = title_tag['href'] if title_tag else ""
                press_tag = item.select_one("span.txt_info") or item.select_one("span.el-info")
                press = press_tag.get_text(strip=True) if press_tag else "ì–¸ë¡ ì‚¬"
                summary_tag = item.select_one("p.conts-desc") or item.select_one("div.el-desc")
                summary = summary_tag.get_text(strip=True) if summary_tag else "ìš”ì•½ ì—†ìŒ"
                date_text_tag = item.select_one("span.gem-subinfo span.txt_info")
                date_text = date_text_tag.get_text(strip=True) if date_text_tag else "ë‚ ì§œë¶ˆëª…"
                
                date_obj = None
                if date_text != "ë‚ ì§œë¶ˆëª…":
                    now = datetime.now()
                    if re.match(r'\d{4}\.\d{2}\.\d{2}', date_text):
                        date_obj = datetime.strptime(date_text[:10], '%Y.%m.%d')
                    else:
                        try:
                            if "ë¶„ì „" in date_text:
                                minutes = int(re.search(r'(\d+)', date_text).group(1))
                                date_obj = now - timedelta(minutes=minutes)
                            elif "ì‹œê°„ì „" in date_text:
                                hours = int(re.search(r'(\d+)', date_text).group(1))
                                date_obj = now - timedelta(hours=hours)
                            elif "ì–´ì œ" in date_text:
                                date_obj = now - timedelta(days=1)
                        except: date_obj = None
                
                final_date = date_obj.strftime('%Y.%m.%d') if date_obj else "ë‚ ì§œë¶ˆëª…"
                article_list.append({"title": title, "press": press, "summary": summary, "link": link, "date": final_date, "date_obj": date_obj})
        except Exception as e:
            st.error(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    return article_list

def get_full_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"}
        res = requests.get(url, headers=headers, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        content_area = soup.select_one('section[dmcf-sid]') or soup.select_one('.article_view') or soup.select_one('#harmonyContainer') or soup.select_one('article')
        if content_area:
            paragraphs = content_area.find_all(['p', 'br'])
            text_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            return "\n\n".join(text_lines)
        return "ë³¸ë¬¸ ì˜ì—­ì°¾ê¸°ì— ì‹¤íŒ¨í–ˆì–´ìš” ğŸ”—"
    except Exception as e:
        return f"ë³¸ë¬¸ ë¶ˆëŸ¬ì˜¤ê¸° ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}"

# --- ë©”ì¸ ë¡œì§ ---
if 'filtered_df' not in st.session_state:
    st.session_state['filtered_df'] = None
if 'expanded_idx' not in st.session_state:
    st.session_state['expanded_idx'] = None

if st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘! ğŸš€"):
    if len(date_range) != 2:
        st.warning("ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”!")
    else:
        start_date, end_date = date_range
        start_dt = datetime.combine(start_date, datetime.min.time())
        end_dt = datetime.combine(end_date, datetime.max.time())

        with st.spinner('ë‰´ìŠ¤ë¥¼ ê¸ì–´ì˜¤ëŠ” ì¤‘...'):
            all_data = crawl_news(keyword, max_pages)
            if all_data:
                filtered_data = [a for a in all_data if a['date_obj'] and start_dt <= a['date_obj'] <= end_dt]
                if filtered_data:
                    st.session_state['filtered_df'] = pd.DataFrame(filtered_data).drop_duplicates(subset=['title'])
                else:
                    st.warning("í•´ë‹¹ ë²”ìœ„ì— ë‰´ìŠ¤ê°€ ì—†ì–´ìš” ğŸ˜¢")
            else:
                st.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨!")

# --- ê²°ê³¼ ì¶œë ¥ ë¡œì§ ---
if st.session_state['filtered_df'] is not None:
    df = st.session_state['filtered_df']
    st.success(f"ì´ {len(df)}ê°œì˜ ê³ ìœ  ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! ğŸ‰")
    
    for idx, row in df.iterrows():
        is_expanded = (st.session_state['expanded_idx'] == idx)
        
        with st.expander(f"[{row['date']}] [{row['press']}] - {row['title']}", expanded=is_expanded):
            st.write(row['summary'])
            st.write(f"ğŸ”— [ì›ë¬¸ ë§í¬ ë°”ë¡œê°€ê¸°]({row['link']})")
               
            # ë²„íŠ¼ 1: ë³¸ë¬¸ ê¸ì–´ì˜¤ê¸°
            if st.button("ìƒì„¸ ë‚´ìš© ì „ì²´ ë³´ê¸° ğŸ“–", key=f"btn_{idx}"):
                st.session_state['expanded_idx'] = idx
                with st.spinner('ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...'):
                    st.session_state[f'content_{idx}'] = get_full_content(row['link'])
                st.rerun()

            # ë³¸ë¬¸ ë‚´ìš© í‘œì‹œ
            if f'content_{idx}' in st.session_state and is_expanded:
                st.markdown("---")
                st.info(st.session_state[f'content_{idx}'])
                
                # ë²„íŠ¼ 2: AI ë‚˜ë ˆì´ì…˜ ê°€ê³µ
                st.subheader("ğŸ™ï¸ AI ë‚˜ë ˆì´ì…˜ ê°€ê³µ")
                if st.button("AI ë‚˜ë ˆì´ì…˜ ìƒì„± ì‹œì‘! âœ¨", key=f"ai_{idx}"):
                    if not google_api_key:
                        st.warning("ì‚¬ì´ë“œë°”ì— API í‚¤ë¥¼ ë„£ì–´ì£¼ì„¸ìš”! ğŸ”‘")
                    else:
                        with st.spinner('Gemini AIê°€ ê°€ê³µí•˜ëŠ” ì¤‘...'):
                            narration = generate_narration(google_api_key, st.session_state[f'content_{idx}'])
                            st.session_state[f'narration_{idx}'] = narration
                        st.rerun()

                # AI ê²°ê³¼ í‘œì‹œ
                if f'narration_{idx}' in st.session_state:
                    st.write(st.session_state[f'narration_{idx}'])