import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta
import re

st.set_page_config(page_title="ë‰´ìŠ¤ ì •ë¦¬ë´‡", page_icon="ğŸ› ï¸")

# ì œëª©
st.title(" ë‰´ìŠ¤ ìš”ì•½ê¸°")
st.write("ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì„œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    keyword = st.text_input("í‚¤ì›Œë“œ", value="ì‚¼ì„±ë¼ì´ì˜¨ì¦ˆ")
    max_pages = st.number_input("ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜", min_value=1, max_value=10, value=3)
    st.divider()
    st.info("Gemini API í‚¤ë¥¼ ë„£ìœ¼ë©´ ìš”ì•½ ê¸°ëŠ¥ì„ ì“¸ ìˆ˜ ìˆì–´ìš”! (ì—…ë°ì´íŠ¸ ì˜ˆì •)")

# í¬ë¡¤ë§
def crawl_news(keyword, pages):
    article_list = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    
    for page in range(1, pages + 1):
        # ì •í™•ë„ìˆœ/ê¸°ê°„ ì˜µì…˜ ì¶”ê°€í•˜ì
        url = f"https://search.daum.net/search?w=news&q={keyword}&p={page}"
        
        try:
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status() # ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
            soup = BeautifulSoup(response.text, 'html.parser')
            
            news_items = soup.find_all('li', {'data-docid': True}) or soup.select("ul.c-list-basic > li")
            
            if not news_items:
                news_items = soup.select("div.item-bundle-news")

            for item in news_items:
                # ì œëª© ì¶”ì¶œ
                title_tag = item.select_one("div.item-title strong.tit-g a") or item.select_one("a.el-title")
                title = title_tag.get_text(strip=True) if title_tag else ""
                
                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                press_tag = item.select_one("span.txt_info") or item.select_one("span.el-info")
                press = press_tag.get_text(strip=True) if press_tag else "ì–¸ë¡ ì‚¬"
                
                # ìš”ì•½ ì¶”ì¶œ
                summary_tag = item.select_one("p.conts-desc") or item.select_one("div.el-desc")
                summary = summary_tag.get_text(strip=True) if summary_tag else "ìš”ì•½ ì—†ìŒ"
                
                if title: # ì œëª©ì´ ìˆëŠ” ê²ƒë§Œ ì €ì¥
                    article_list.append({"title": title, "press": press, "summary": summary})
            
            time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€
        except Exception as e:
            st.error(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
            
    return article_list

# ì‹¤í–‰ ë²„íŠ¼
if st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘! ğŸš€"):
    with st.spinner('ë‰´ìŠ¤ë¥¼ ê¸ì–´ì˜¤ëŠ” ì¤‘... ì ì‹œë§Œìš”! â˜•'):
        data = crawl_news(keyword, max_pages)
        
        if data:
            df = pd.DataFrame(data)
            # ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['title'])
            st.success(f"ì´ {len(df)}ê°œì˜ ê³ ìœ  ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! ğŸ‰")
            
            # ê²°ê³¼ ì¶œë ¥
            for idx, row in df.iterrows():
                with st.expander(f"[{row['press']}] {row['title']}"):
                    st.write(row['summary'])
        else:
            st.error("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆì–´ìš”. ğŸ˜¢")