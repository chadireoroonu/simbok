import streamlit as st
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime, timedelta
import re

st.set_page_config(page_title="ë‰´ìŠ¤ ì •ë¦¬ë´‡", page_icon="ğŸ› ï¸")

# ì œëª©
st.title(" ë‰´ìŠ¤ ìš”ì•½ê¸°")
st.write("ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•´ì„œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ ë“œë¦½ë‹ˆë‹¤!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ì„¤ì •")
    keyword = st.text_input("í‚¤ì›Œë“œ", value="í‚¤ì›Œë“œ")
    max_pages = st.number_input("ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜", min_value=1, max_value=10, value=3)
    st.divider()
    st.subheader("ğŸ“… ì¡°íšŒ ê¸°ê°„ ì„¤ì •")
    # ì‚¬ìš©ìê°€ ë‹¬ë ¥ì—ì„œ ë‚ ì§œ ë²”ìœ„ë¥¼ ì„ íƒí•˜ê²Œ í•©ë‹ˆë‹¤.
    today = datetime.now()
    seven_days_ago = today - timedelta(days=7)
    date_range = st.date_input(
        "ì¡°íšŒ ì‹œì‘ì¼ - ì¢…ë£Œì¼",
        value=(seven_days_ago, today),
        max_value=today
    )
    st.info("ì„ íƒí•œ ë‚ ì§œ ë‚´ì˜ ê¸°ì‚¬ë§Œ í‘œì‹œë©ë‹ˆë‹¤. ğŸ•’")


# í¬ë¡¤ë§
def crawl_news(keyword, pages):
    article_list = []
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    
    for page in range(1, pages + 1):
        # ì •í™•ë„ìˆœ/ê¸°ê°„ ì˜µì…˜ ì¶”ê°€í•˜ì
        url = f"https://search.daum.net/search?w=news&q={keyword}&p={page}&f=sort&sort=rec"
        
        try:
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status() # ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬
            soup = BeautifulSoup(response.text, 'html.parser')
            
            news_items = soup.find_all('li', {'data-docid': True}) or soup.select("ul.c-list-basic > li")
            
            # if not news_items:
            #     news_items = soup.select("div.item-bundle-news")

            for item in news_items:
                # ì œëª© ì¶”ì¶œ
                title_tag = item.select_one("div.item-title strong.tit-g a") or item.select_one("a.el-title")
                title = title_tag.get_text(strip=True) if title_tag else ""
                
                # ë§í¬ ì¶”ì¶œ
                link = title_tag['href']
                
                # ì–¸ë¡ ì‚¬ ì¶”ì¶œ
                press_tag = item.select_one("span.txt_info") or item.select_one("span.el-info")
                press = press_tag.get_text(strip=True) if press_tag else "ì–¸ë¡ ì‚¬"
                
                # ìš”ì•½ ì¶”ì¶œ
                summary_tag = item.select_one("p.conts-desc") or item.select_one("div.el-desc")
                summary = summary_tag.get_text(strip=True) if summary_tag else "ìš”ì•½ ì—†ìŒ"

                # ë‚ ì§œ ì²˜ë¦¬
                date_text_tag = item.select_one("span.gem-subinfo span.txt_info")
                date_text = date_text_tag.get_text(strip=True) if date_text_tag else "ë‚ ì§œë¶ˆëª…"
                
                date_obj = None # datetime ê°ì²´ ë³€í™˜ í›„ ë¹„êµ
                
                if date_text != "ë‚ ì§œë¶ˆëª…":
                    now = datetime.now()
                    if re.match(r'\d{4}\.\d{2}\.\d{2}', date_text):
                        # '2024.05.20' í˜•ì‹ì„ datetime ê°ì²´ë¡œ ë³€í™˜
                        date_obj = datetime.strptime(date_text[:10], '%Y.%m.%d')
                    else:
                        try:
                            if "ë¶„ì „" in date_text:
                                minutes = int(re.search(r'(\d+)', date_text).group(1))
                                date_obj = now - timedelta(minutes=minutes)
                            elif "ì‹œê°„ì „" in date_text:
                                hours = int(re.search(r'(\d+)', date_text).group(1))
                                date_obj = now - timedelta(hours=hours)
                            elif "ì–´ì œ" in date_text:
                                date_obj = now - timedelta(days=1)
                        except:
                            date_obj = None
                
                # ë‚ ì§œ ë¬¸ìì—´ ì €ì¥
                final_date = date_obj.strftime('%Y.%m.%d') if date_obj else "ë‚ ì§œë¶ˆëª…"
                
                article_list.append({
                    "title": title, 
                    "press": press,
                    "summary": summary,
                    "link": link,
                    "date": final_date,
                    "date_obj": date_obj # í•„í„°ë§ìš©
                })
            # time.sleep(0.1) # ì„œë²„ ë¶€í•˜ ë°©ì§€
        except Exception as e:
            st.error(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
            
    return article_list

# ì‹¤í–‰ ë²„íŠ¼
if st.button("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘! ğŸš€"):
    if len(date_range) != 2:
        st.warning("ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ëª¨ë‘ ì„ íƒí•´ ì£¼ì„¸ìš”! ğŸ“…")
    else:
        start_date, end_date = date_range
        # ì„ íƒí•œ ë‚ ì§œë¥¼ ë¹„êµ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜ (ì‹œê°„ ì •ë³´ ì œê±°)
        start_dt = datetime.combine(start_date, datetime.min.time())
        end_dt = datetime.combine(end_date, datetime.max.time())

        with st.spinner('ë‰´ìŠ¤ë¥¼ ê¸ì–´ì˜¤ëŠ” ì¤‘... ì ì‹œë§Œìš”! â˜•'):
            all_data = crawl_news(keyword, max_pages)
            
            if all_data:
                # ë‚ ì§œ ë²”ìœ„ ë‚´ í•„í„°ë§
                filtered_data = [
                    a for a in all_data 
                    if a['date_obj'] and start_dt <= a['date_obj'] <= end_dt
                ]
                
                df = pd.DataFrame(filtered_data).drop_duplicates(subset=['title'])
                
                if not df.empty:
                    st.success(f"ğŸ“… {start_date} ~ {end_date} ì‚¬ì´ì— ì´ {len(df)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    for idx, row in df.iterrows():
                        with st.expander(f"[{row['date']}] [{row['press']}] - {row['title']}"):
                            st.write(row['summary'])
                            st.write(f"ğŸ”— [ì›ë¬¸ ë§í¬ ë°”ë¡œê°€ê¸°]({row['link']})")
                else:
                    st.warning("í•´ë‹¹ ë‚ ì§œ ë²”ìœ„ì— ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ì–´ìš”. ğŸ˜¢")
            else:
                st.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨! ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")